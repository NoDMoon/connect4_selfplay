{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# поле 7х7, игроки поочериди выбирают колонку, в которой добавляется клетка их цвета. \n",
    "# задача игрока - образовать линию из 4 клеток своего увета по вертикали, горизонтали или диагонали"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install gym\n",
    "#!pip install matplotlib\n",
    "#!pip install ray\n",
    "#!pip install ray[tune]\n",
    "#!pip install tensorflow\n",
    "#!pip install tqdm\n",
    "#!pip install dm_tree\n",
    "#!pip install scikit-learn\n",
    "#!pip install scikit-image\n",
    "#!pip install lz4\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f3b_6Ukfqc7z"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sDY5fJcGtxmz"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "#import ray.rllib.algorithms.dqn as dqn\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vFk5f3nzIH9I"
   },
   "outputs": [],
   "source": [
    "n_cols = 7\n",
    "\n",
    "max_turns = n_cols**2//2+1 # 24-25 ходов (49 клеток, 2 игрока)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VBKhDPHn7O9W"
   },
   "outputs": [],
   "source": [
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2 as base_model\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork as fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zDbkXxjfoCUl"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kzpeq0mLA4TD"
   },
   "outputs": [],
   "source": [
    "def register_env(env, env_name, env_config={}):\n",
    "    tune.register_env(env_name, lambda env_name: env(env_config=env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wm4_x1hgPuTq"
   },
   "outputs": [],
   "source": [
    "def default_state():\n",
    "    state=dict()\n",
    "    state['mask'] = np.ones(n_cols)\n",
    "    state['obs'] = np.zeros(n_cols**2, dtype=int)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "87Sb7cRno7kF"
   },
   "outputs": [],
   "source": [
    "def checkmate (desk, col, pos):\n",
    "    # -1 - игра продолжается\n",
    "    #  0 - ничья\n",
    "    #  1 - победа\n",
    "    res = -1\n",
    "    comb = [1]*4\n",
    "    \n",
    "    # вертикаль - можем смотреть ток вниз\n",
    "    if pos >=3:\n",
    "        if np.all(desk[col, pos-3:pos+1] == comb):\n",
    "            return 1\n",
    "    \n",
    "    # горизонталь - до пердела поля\n",
    "    #r = n_cols - pos - 1\n",
    "    #l = pos - 1\n",
    "    for i in range(4):\n",
    "        if col - i < 0 or col+3+1-i > n_cols: pass\n",
    "        elif np.all(desk[col-i:col+3+1-i, pos] == comb):\n",
    "            return 1\n",
    "        \n",
    "    # диагональ - все 4 стороны\n",
    "    for i in range(4):\n",
    "        if col - i < 0 or col+3+1-i > n_cols: pass\n",
    "        elif pos - i < 0 or pos+3+1-i > n_cols: pass\n",
    "        elif np.all(desk[col-i:col+3+1-i, pos-i:pos+3+1-i] == comb):\n",
    "            return 1\n",
    "    #print((desk==0).sum())\n",
    "    if (desk==0).sum() == 0:\n",
    "        res = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkmate (desk, col, pos):\n",
    "    # -1 - игра продолжается\n",
    "    #  0 - ничья\n",
    "    #  1 - победа\n",
    "    res = -1\n",
    "    comb = [1]*4\n",
    "    \n",
    "    # вертикаль - можем смотреть ток вниз\n",
    "    if pos >=3:\n",
    "        if np.all(desk[col, pos-3:pos+1] == comb):\n",
    "            return 1\n",
    "    \n",
    "    # горизонталь - до пердела поля\n",
    "    #r = n_cols - pos - 1\n",
    "    #l = pos - 1\n",
    "    for i in range(4):\n",
    "        if col - i < 0 or col+3+1-i > n_cols: pass\n",
    "        elif np.all(desk[col-i:col+3+1-i, pos] == comb):\n",
    "            return 1\n",
    "        \n",
    "    # диагональ - все 4 стороны\n",
    "    for i in range(4):\n",
    "        if   col - i   < 0 or col+3+1-i > n_cols: pass\n",
    "        elif pos - i   < 0 or pos+3+1-i > n_cols: pass\n",
    "        elif col-(3-i) < 0 or col+i+1   > n_cols: pass\n",
    "        elif pos-(3-i) < 0 or pos+i+1   > n_cols: pass\n",
    "        elif np.all(desk[range(col-i,col+(3-i)+1), range(pos-i,pos+(3-i)+1)] == comb) \\\n",
    "          or np.all(desk[range(col-(3-i),col+i+1), range(pos-(3-i),pos+i+1)] == comb):\n",
    "            return 1\n",
    "    #print((desk==0).sum())\n",
    "    if (desk==0).sum() == 0:\n",
    "        res = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NIJDTdyMuSYv"
   },
   "outputs": [],
   "source": [
    "def place(desk, col):\n",
    "    pos = np.where(desk[col]==0)[0]\n",
    "    #print('pos:', pos)\n",
    "    desk[col][pos[0]] = 1\n",
    "  \n",
    "    check = checkmate(desk, col, pos[0])\n",
    "    #print(desk, '\\n')\n",
    "    \n",
    "    return desk, check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch(desk):\n",
    "    desk = deepcopy(desk)\n",
    "    desk[desk ==  1] = -1\n",
    "    desk[desk ==  2] =  1\n",
    "    desk[desk == -1] =  2\n",
    "    return desk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mXhqJWwIKY8w"
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Box, Tuple, Dict, Discrete, MultiDiscrete, MultiBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, env_config={}):\n",
    "        self.action_space = Discrete(n_cols)\n",
    "        self.observation_space = Dict({'mask': MultiBinary(n_cols),\n",
    "                                       'obs':  MultiDiscrete([3]*n_cols**2)})\n",
    "        self.reward = 0\n",
    "        self.done =   False\n",
    "        self.state =  default_state()\n",
    "        \n",
    "        self.opp = tf.keras.Sequential([\n",
    "                   tf.keras.layers.Dense(256, activation='sigmoid',input_shape = (147,)),\n",
    "                   tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "                   #tf.keras.layers.Dense(n_cols),\n",
    "                   tf.keras.layers.Dense(n_cols, activation='softmax'),    \n",
    "                   ])\n",
    "        self.pp = ray.rllib.models.preprocessors.OneHotPreprocessor(MultiDiscrete([3]*n_cols**2))\n",
    "\n",
    "    def update_weights(self, weights):\n",
    "        self.opp.set_weights(weights)\n",
    "        \n",
    "    def make_opposite_action(self, obs, mask):\n",
    "        #print(self.pp.transform(switch(obs).flatten()))\n",
    "\n",
    "        act_probs = self.opp(self.pp.transform(switch(obs).flatten()).reshape(1, -1))\n",
    "        #print('act_probs:', act_probs)\n",
    "        \n",
    "        avail_acts = np.arange(n_cols)[mask==1]\n",
    "        #print('avail_acts:', avail_acts)\n",
    "        \n",
    "        avail_probs = act_probs.numpy().flatten()[mask==1]\n",
    "        #print('avail_probs:', avail_probs)\n",
    "        \n",
    "        act_probs_avail = np.exp(avail_probs)/np.sum(np.exp(avail_probs))\n",
    "        #print('act_probs_avail', list(act_probs_avail))\n",
    "        \n",
    "        act = np.random.choice(avail_acts, p=act_probs_avail)\n",
    "        #act = avail_acts[np.argmax(avail_probs)]\n",
    "        #print('act',act)\n",
    "        \n",
    "        \n",
    "        #avail_acts = np.arange(n_cols)[self.state['mask']==1]\n",
    "        #act = np.random.choice(avail_acts)\n",
    "        return act\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.state = default_state()\n",
    "\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self.reward = 0\n",
    "        #print('act1:', action)\n",
    "        obs = self.state['obs'].reshape(n_cols, n_cols)\n",
    "\n",
    "        obs, check = place(obs, action)\n",
    "        if check != -1: \n",
    "            self.done=True\n",
    "            if check == 1:self.reward = 1\n",
    "            else: self.reward = 0\n",
    "\n",
    "        #print('mask before act1:', self.state['mask'])\n",
    "        if (obs[action] != 0).all():\n",
    "            self.state['mask'][action] = 0\n",
    "        #print('mask after act1:', self.state['mask'])\n",
    "        #print(obs)\n",
    "        #print()\n",
    "            \n",
    "\n",
    "        if not self.done:\n",
    "\n",
    "            act2 = self.make_opposite_action(obs, self.state['mask'])\n",
    "            #print(act2)\n",
    "           \n",
    "            obs, check = place(switch(obs), act2)\n",
    "            obs = switch(obs)\n",
    "            #print(obs)\n",
    "            \n",
    "            if check != -1: \n",
    "                self.done=True\n",
    "                if check == 1:self.reward = -1\n",
    "                else: self.reward = 0\n",
    "                    \n",
    "            #print('mask before act2:', self.state['mask'])\n",
    "            if (obs[act2] != 0).all():\n",
    "                self.state['mask'][act2] = 0\n",
    "            #print('mask after act2:', self.state['mask'])\n",
    "\n",
    "        self.state['obs'] =  obs.flatten()\n",
    "        #print()\n",
    "        return self.state, self.reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Nu_D2Usf7Zty"
   },
   "outputs": [],
   "source": [
    "class Agent(base_model):\n",
    "    def __init__(self, obs_space, action_space, num_outputs,\n",
    "        model_config, name, true_obs_shape=(n_cols**2*3,),\n",
    "        action_embed_size=n_cols, *args, **kwargs):\n",
    "        \n",
    "        super(Agent, self).__init__(obs_space,\n",
    "            action_space, num_outputs, model_config, name, \n",
    "            *args, **kwargs)\n",
    "        \n",
    "        self.action_embed_model = fcn(\n",
    "            gym.spaces.Box(-1, 1, shape=true_obs_shape, dtype=int), \n",
    "                action_space, action_embed_size,\n",
    "            model_config, name + \"_action_embedding\")\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        #print(type(input_dict))\n",
    "        action_mask = input_dict[\"obs\"][\"mask\"]\n",
    "\n",
    "        # Compute the predicted action embedding\n",
    "        action_embed, _ = self.action_embed_model({\n",
    "            \"obs\": input_dict[\"obs\"][\"obs\"]})\n",
    "\n",
    "        # Mask out invalid actions \n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "        return action_embed + inf_mask, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "h9sMnoZdpEcJ"
   },
   "outputs": [],
   "source": [
    "def train(agent, n_iter, n_report):\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        report = agent.train()\n",
    "        if not (i % n_report):  \n",
    "            \n",
    "            print()\n",
    "            print('mean episode length:', report['episode_len_mean'])\n",
    "            print('max episode reward:',  report['episode_reward_max'])\n",
    "            print('mean episode reward:', report['episode_reward_mean'])\n",
    "            print('min episode reward:',  report['episode_reward_min'])\n",
    "            print('total episodes:',      report['episodes_total'])\n",
    "            print()\n",
    "\n",
    "            checkpoint = agent.save()\n",
    "            #print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rtqn2TquzlUO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 15:54:05,163\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "2022-11-28 15:54:06,242\tWARNING ppo.py:350 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=6 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 667.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13680)\u001b[0m 2022-11-28 15:54:09,564\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[2m\u001b[36m(RolloutWorker pid=13680)\u001b[0m /home/zfear/Documents/software/jupyter/lib/python3.8/site-packages/numpy/core/_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13680)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:29<00:00, 29.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 11.927927927927929\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.23723723723723725\n",
      "min episode reward: -1.0\n",
      "total episodes: 333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:29<00:00, 29.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 9.155606407322654\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.28604118993135014\n",
      "min episode reward: -1.0\n",
      "total episodes: 770\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:30<00:30, 30.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 9.866666666666667\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.3012345679012346\n",
      "min episode reward: -1.0\n",
      "total episodes: 1175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [01:00<00:00, 30.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 9.23165137614679\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.3440366972477064\n",
      "min episode reward: -1.0\n",
      "total episodes: 1611\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:30<00:30, 30.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 9.404255319148936\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.3309692671394799\n",
      "min episode reward: -1.0\n",
      "total episodes: 2034\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [01:01<00:00, 30.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.814004376367615\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.4573304157549234\n",
      "min episode reward: -1.0\n",
      "total episodes: 2491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 1/3 [00:30<01:01, 30.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.957399103139014\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.42376681614349776\n",
      "min episode reward: -1.0\n",
      "total episodes: 2937\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████               | 2/3 [01:00<00:30, 30.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.517021276595745\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.48936170212765956\n",
      "min episode reward: -1.0\n",
      "total episodes: 3407\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [01:30<00:00, 30.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.716483516483516\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.46813186813186813\n",
      "min episode reward: -1.0\n",
      "total episodes: 3862\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 1/3 [00:29<00:59, 29.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.609848484848484\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5606060606060606\n",
      "min episode reward: -1.0\n",
      "total episodes: 4390\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████               | 2/3 [01:00<00:30, 30.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.5668549905838045\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5178907721280602\n",
      "min episode reward: -1.0\n",
      "total episodes: 4921\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [01:29<00:00, 29.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.190647482014389\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5827338129496403\n",
      "min episode reward: -1.0\n",
      "total episodes: 5477\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████▎                                 | 1/4 [00:29<01:29, 29.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.588235294117647\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5635673624288425\n",
      "min episode reward: -1.0\n",
      "total episodes: 6004\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 2/4 [00:59<00:59, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.036\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.46\n",
      "min episode reward: -1.0\n",
      "total episodes: 6504\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████▊           | 3/4 [01:30<00:30, 30.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.191358024691358\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.4074074074074074\n",
      "min episode reward: -1.0\n",
      "total episodes: 6990\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [02:01<00:00, 30.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.844054580896686\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5633528265107213\n",
      "min episode reward: -1.0\n",
      "total episodes: 7503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████▎                                 | 1/4 [00:30<01:32, 30.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.008080808080807\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5595959595959596\n",
      "min episode reward: -1.0\n",
      "total episodes: 7998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 2/4 [01:00<01:00, 30.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.385416666666666\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.4875\n",
      "min episode reward: -1.0\n",
      "total episodes: 8478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████▊           | 3/4 [01:31<00:30, 30.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.140243902439025\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.47560975609756095\n",
      "min episode reward: -1.0\n",
      "total episodes: 8970\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [02:02<00:00, 30.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.854616895874264\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.48919449901768175\n",
      "min episode reward: -1.0\n",
      "total episodes: 9479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████                                    | 1/5 [00:31<02:06, 31.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.938492063492063\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5674603174603174\n",
      "min episode reward: -1.0\n",
      "total episodes: 9983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████                           | 2/5 [01:02<01:34, 31.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.98807157057654\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5467196819085487\n",
      "min episode reward: -1.0\n",
      "total episodes: 10486\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████                  | 3/5 [01:33<01:01, 30.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.022132796780683\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5372233400402414\n",
      "min episode reward: -1.0\n",
      "total episodes: 10983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████         | 4/5 [02:03<00:30, 30.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.002\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.524\n",
      "min episode reward: -1.0\n",
      "total episodes: 11483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [02:34<00:00, 30.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.200819672131148\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5368852459016393\n",
      "min episode reward: -1.0\n",
      "total episodes: 11971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████                                    | 1/5 [00:31<02:04, 31.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.8858267716535435\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5433070866141733\n",
      "min episode reward: -1.0\n",
      "total episodes: 12479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████                           | 2/5 [01:01<01:31, 30.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.883858267716535\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5866141732283464\n",
      "min episode reward: -1.0\n",
      "total episodes: 12987\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████                  | 3/5 [01:31<01:00, 30.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.8101761252446185\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.49510763209393344\n",
      "min episode reward: -1.0\n",
      "total episodes: 13498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████         | 4/5 [02:00<00:30, 30.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.261855670103094\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.43917525773195876\n",
      "min episode reward: -1.0\n",
      "total episodes: 13983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [02:31<00:00, 30.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.99003984063745\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5\n",
      "min episode reward: -1.0\n",
      "total episodes: 14485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████▌                                     | 1/6 [00:30<02:31, 30.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.066801619433198\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5182186234817814\n",
      "min episode reward: -1.0\n",
      "total episodes: 14979\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 2/6 [01:00<02:00, 30.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 8.208163265306123\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5102040816326531\n",
      "min episode reward: -1.0\n",
      "total episodes: 15469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 3/6 [01:30<01:29, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 5.524861878453039\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7928176795580111\n",
      "min episode reward: -1.0\n",
      "total episodes: 16193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████               | 4/6 [01:59<00:59, 29.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 5.649717514124294\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7259887005649718\n",
      "min episode reward: -1.0\n",
      "total episodes: 16901\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████▌       | 5/6 [02:29<00:29, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 5.608391608391608\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.737062937062937\n",
      "min episode reward: -1.0\n",
      "total episodes: 17616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 6/6 [02:59<00:00, 29.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 5.5760111576011155\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7712691771269177\n",
      "min episode reward: -1.0\n",
      "total episodes: 18333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████▌                                     | 1/6 [00:29<02:29, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.9119170984455955\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5924006908462867\n",
      "min episode reward: -1.0\n",
      "total episodes: 18912\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 2/6 [00:59<01:59, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.923611111111111\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5486111111111112\n",
      "min episode reward: -1.0\n",
      "total episodes: 19488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 3/6 [01:29<01:29, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.662229617304493\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.56738768718802\n",
      "min episode reward: -1.0\n",
      "total episodes: 20089\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████               | 4/6 [01:59<00:59, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.8270547945205475\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5924657534246576\n",
      "min episode reward: -1.0\n",
      "total episodes: 20673\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████▌       | 5/6 [02:29<00:29, 29.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.958477508650519\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5467128027681661\n",
      "min episode reward: -1.0\n",
      "total episodes: 21251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 6/6 [02:59<00:00, 29.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.911611785095321\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5909878682842288\n",
      "min episode reward: -1.0\n",
      "total episodes: 21828\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████▍                                      | 1/7 [00:29<02:58, 29.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.783417935702199\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6311336717428088\n",
      "min episode reward: -1.0\n",
      "total episodes: 22419\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▊                                | 2/7 [00:59<02:28, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.165775401069519\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5365418894830659\n",
      "min episode reward: -1.0\n",
      "total episodes: 22980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████▎                         | 3/7 [01:29<01:59, 29.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.901384083044983\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5674740484429066\n",
      "min episode reward: -1.0\n",
      "total episodes: 23558\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████▋                   | 4/7 [01:58<01:29, 29.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.714765100671141\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6073825503355704\n",
      "min episode reward: -1.0\n",
      "total episodes: 24154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████▏            | 5/7 [02:28<00:59, 29.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.954782608695652\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.568695652173913\n",
      "min episode reward: -1.0\n",
      "total episodes: 24729\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████▌      | 6/7 [02:58<00:29, 29.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.924006908462867\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5682210708117443\n",
      "min episode reward: -1.0\n",
      "total episodes: 25308\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [03:28<00:00, 29.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.9859894921190895\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.4956217162872154\n",
      "min episode reward: -1.0\n",
      "total episodes: 25879\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████▍                                      | 1/7 [00:30<03:00, 30.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.941278065630398\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5233160621761658\n",
      "min episode reward: -1.0\n",
      "total episodes: 26458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▊                                | 2/7 [00:59<02:29, 29.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.994746059544658\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5376532399299475\n",
      "min episode reward: -1.0\n",
      "total episodes: 27029\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████▎                         | 3/7 [01:29<01:59, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.809523809523809\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5816326530612245\n",
      "min episode reward: -1.0\n",
      "total episodes: 27617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████▋                   | 4/7 [01:59<01:29, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.695652173913044\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5752508361204013\n",
      "min episode reward: -1.0\n",
      "total episodes: 28215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████▏            | 5/7 [02:29<00:59, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.960069444444445\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5798611111111112\n",
      "min episode reward: -1.0\n",
      "total episodes: 28791\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|██████████████████████████████████████▌      | 6/7 [02:59<00:29, 29.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.014059753954306\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5325131810193322\n",
      "min episode reward: -1.0\n",
      "total episodes: 29360\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [03:29<00:00, 29.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.006980802792321\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5427574171029669\n",
      "min episode reward: -1.0\n",
      "total episodes: 29933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████▋                                       | 1/8 [00:30<03:30, 30.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.952961672473868\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.578397212543554\n",
      "min episode reward: -1.0\n",
      "total episodes: 30507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████▎                                 | 2/8 [01:00<03:00, 30.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.819420783645656\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5468483816013628\n",
      "min episode reward: -1.0\n",
      "total episodes: 31094\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████▉                            | 3/8 [01:29<02:29, 29.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.601652892561983\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5867768595041323\n",
      "min episode reward: -1.0\n",
      "total episodes: 31699\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 4/8 [02:00<02:00, 30.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.858119658119658\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5145299145299145\n",
      "min episode reward: -1.0\n",
      "total episodes: 32284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████▏                | 5/8 [02:30<01:30, 30.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.814310051107325\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6524701873935264\n",
      "min episode reward: -1.0\n",
      "total episodes: 32871\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████▊           | 6/8 [03:00<01:00, 30.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.759729272419627\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6006768189509306\n",
      "min episode reward: -1.0\n",
      "total episodes: 33462\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████▍     | 7/8 [03:30<00:30, 30.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.751260504201681\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5865546218487395\n",
      "min episode reward: -1.0\n",
      "total episodes: 34057\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [04:00<00:00, 30.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.028169014084507\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6267605633802817\n",
      "min episode reward: -1.0\n",
      "total episodes: 34625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████▋                                       | 1/8 [00:29<03:27, 29.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.056537102473499\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.519434628975265\n",
      "min episode reward: -1.0\n",
      "total episodes: 35191\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████▎                                 | 2/8 [00:59<02:58, 29.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.132978723404255\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5177304964539007\n",
      "min episode reward: -1.0\n",
      "total episodes: 35755\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████▉                            | 3/8 [01:29<02:29, 29.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.083185840707965\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5575221238938053\n",
      "min episode reward: -1.0\n",
      "total episodes: 36320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 4/8 [01:59<01:59, 29.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.154121863799283\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5161290322580645\n",
      "min episode reward: -1.0\n",
      "total episodes: 36878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████▏                | 5/8 [02:29<01:29, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.951388888888889\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5972222222222222\n",
      "min episode reward: -1.0\n",
      "total episodes: 37454\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████▊           | 6/8 [02:59<00:59, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.056338028169014\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5246478873239436\n",
      "min episode reward: -1.0\n",
      "total episodes: 38022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████▍     | 7/8 [03:29<00:29, 29.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.961605584642234\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5776614310645725\n",
      "min episode reward: -1.0\n",
      "total episodes: 38595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [04:00<00:00, 30.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.128342245989304\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5151515151515151\n",
      "min episode reward: -1.0\n",
      "total episodes: 39156\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████                                        | 1/9 [00:30<04:01, 30.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.741176470588235\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6235294117647059\n",
      "min episode reward: -1.0\n",
      "total episodes: 39751\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████                                   | 2/9 [01:00<03:29, 29.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.040421792618629\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5149384885764499\n",
      "min episode reward: -1.0\n",
      "total episodes: 40320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 3/9 [01:29<02:59, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 4.954207920792079\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.8564356435643564\n",
      "min episode reward: -1.0\n",
      "total episodes: 41128\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████                         | 4/9 [01:59<02:29, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 4.957868649318463\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.838909541511772\n",
      "min episode reward: -1.0\n",
      "total episodes: 41935\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████                    | 5/9 [02:29<01:59, 29.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 5.094267515923567\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7936305732484077\n",
      "min episode reward: -1.0\n",
      "total episodes: 42720\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████               | 6/9 [03:00<01:30, 30.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 5.004993757802747\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.8551810237203495\n",
      "min episode reward: -1.0\n",
      "total episodes: 43521\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████          | 7/9 [03:31<01:01, 30.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 5.11651728553137\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.8309859154929577\n",
      "min episode reward: -1.0\n",
      "total episodes: 44302\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████     | 8/9 [04:02<00:30, 30.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 5.211963589076723\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.8205461638491548\n",
      "min episode reward: -1.0\n",
      "total episodes: 45071\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [04:32<00:00, 30.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 5.031446540880503\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.8415094339622642\n",
      "min episode reward: -1.0\n",
      "total episodes: 45866\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████                                        | 1/9 [00:29<03:59, 29.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.3010948905109485\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6313868613138686\n",
      "min episode reward: -1.0\n",
      "total episodes: 46414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████                                   | 2/9 [00:59<03:28, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.287795992714026\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6539162112932605\n",
      "min episode reward: -1.0\n",
      "total episodes: 46963\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████                              | 3/9 [01:29<02:58, 29.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.005272407732865\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6449912126537786\n",
      "min episode reward: -1.0\n",
      "total episodes: 47532\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████                         | 4/9 [01:59<02:30, 30.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.175313059033989\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7030411449016101\n",
      "min episode reward: -1.0\n",
      "total episodes: 48091\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████                    | 5/9 [02:29<01:59, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.323529411764706\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6617647058823529\n",
      "min episode reward: -1.0\n",
      "total episodes: 48635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████               | 6/9 [02:59<01:29, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.221621621621622\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6612612612612613\n",
      "min episode reward: -1.0\n",
      "total episodes: 49190\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████          | 7/9 [03:29<00:59, 29.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.033274956217163\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6987740805604203\n",
      "min episode reward: -1.0\n",
      "total episodes: 49761\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████     | 8/9 [03:59<00:29, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.245009074410163\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6406533575317604\n",
      "min episode reward: -1.0\n",
      "total episodes: 50312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [04:29<00:00, 29.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.134991119005329\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6678507992895204\n",
      "min episode reward: -1.0\n",
      "total episodes: 50875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 1/10 [00:29<04:27, 29.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.125\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6571428571428571\n",
      "min episode reward: -1.0\n",
      "total episodes: 51435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 2/10 [00:59<03:58, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.392265193370166\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5874769797421732\n",
      "min episode reward: -1.0\n",
      "total episodes: 51978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████▏                              | 3/10 [01:29<03:29, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.330275229357798\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6587155963302752\n",
      "min episode reward: -1.0\n",
      "total episodes: 52523\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 4/10 [01:59<02:59, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.328440366972477\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6623853211009174\n",
      "min episode reward: -1.0\n",
      "total episodes: 53068\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 5/10 [02:29<02:29, 29.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.081128747795415\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6331569664902998\n",
      "min episode reward: -1.0\n",
      "total episodes: 53635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████▍                 | 6/10 [02:58<01:59, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.074468085106383\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6773049645390071\n",
      "min episode reward: -1.0\n",
      "total episodes: 54199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████▊             | 7/10 [03:28<01:29, 29.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.207207207207207\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.69009009009009\n",
      "min episode reward: -1.0\n",
      "total episodes: 54754\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████▏        | 8/10 [03:58<00:59, 29.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.994764397905759\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7068062827225131\n",
      "min episode reward: -1.0\n",
      "total episodes: 55327\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████▌    | 9/10 [04:28<00:29, 29.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.351749539594843\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7090239410681399\n",
      "min episode reward: -1.0\n",
      "total episodes: 55870\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [04:58<00:00, 29.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.18850987432675\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.644524236983842\n",
      "min episode reward: -1.0\n",
      "total episodes: 56427\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 1/10 [00:29<04:28, 29.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.288566243194192\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6551724137931034\n",
      "min episode reward: -1.0\n",
      "total episodes: 56978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 2/10 [00:59<03:59, 29.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.311355311355311\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.652014652014652\n",
      "min episode reward: -1.0\n",
      "total episodes: 57524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████▏                              | 3/10 [01:29<03:28, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.256781193490054\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6853526220614828\n",
      "min episode reward: -1.0\n",
      "total episodes: 58077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 4/10 [01:59<02:59, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.204667863554757\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6373429084380611\n",
      "min episode reward: -1.0\n",
      "total episodes: 58634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 5/10 [02:29<02:29, 29.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.88103448275862\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6896551724137931\n",
      "min episode reward: -1.0\n",
      "total episodes: 59214\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████▍                 | 6/10 [02:59<01:59, 29.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.438661710037175\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.587360594795539\n",
      "min episode reward: -1.0\n",
      "total episodes: 59752\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████▊             | 7/10 [03:29<01:29, 29.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.343119266055046\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.636697247706422\n",
      "min episode reward: -1.0\n",
      "total episodes: 60297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████▏        | 8/10 [03:59<00:59, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.116071428571429\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7\n",
      "min episode reward: -1.0\n",
      "total episodes: 60857\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████▌    | 9/10 [04:29<00:29, 29.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.477695167286245\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6319702602230484\n",
      "min episode reward: -1.0\n",
      "total episodes: 61395\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [04:59<00:00, 29.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.263636363636364\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6363636363636364\n",
      "min episode reward: -1.0\n",
      "total episodes: 61945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████                                        | 1/11 [00:29<04:59, 29.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.1263345195729535\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6797153024911032\n",
      "min episode reward: -1.0\n",
      "total episodes: 62507\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████                                    | 2/11 [00:59<04:28, 29.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.266787658802178\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.705989110707804\n",
      "min episode reward: -1.0\n",
      "total episodes: 63058\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████                                | 3/11 [01:29<03:58, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.302197802197802\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6923076923076923\n",
      "min episode reward: -1.0\n",
      "total episodes: 63604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████                            | 4/11 [01:59<03:29, 29.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.247739602169982\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5913200723327305\n",
      "min episode reward: -1.0\n",
      "total episodes: 64157\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████                        | 5/11 [02:29<02:59, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.1553571428571425\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6821428571428572\n",
      "min episode reward: -1.0\n",
      "total episodes: 64717\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████                    | 6/11 [02:59<02:29, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.252268602540835\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6805807622504537\n",
      "min episode reward: -1.0\n",
      "total episodes: 65268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████                | 7/11 [03:28<01:59, 29.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.151785714285714\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6464285714285715\n",
      "min episode reward: -1.0\n",
      "total episodes: 65828\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████            | 8/11 [03:58<01:29, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.290676416819013\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.623400365630713\n",
      "min episode reward: -1.0\n",
      "total episodes: 66375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████        | 9/11 [04:28<00:59, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.335154826958106\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.639344262295082\n",
      "min episode reward: -1.0\n",
      "total episodes: 66924\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████    | 10/11 [04:59<00:29, 29.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.322935779816514\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.5853211009174312\n",
      "min episode reward: -1.0\n",
      "total episodes: 67469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 11/11 [05:28<00:00, 29.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.024604569420035\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6906854130052724\n",
      "min episode reward: -1.0\n",
      "total episodes: 68038\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████                                        | 1/11 [00:29<04:58, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.432282003710575\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6512059369202227\n",
      "min episode reward: -1.0\n",
      "total episodes: 68577\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████                                    | 2/11 [00:59<04:28, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.847602739726027\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6746575342465754\n",
      "min episode reward: -1.0\n",
      "total episodes: 69161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████                                | 3/11 [01:29<03:58, 29.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.079505300353357\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6819787985865724\n",
      "min episode reward: -1.0\n",
      "total episodes: 69727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████                            | 4/11 [01:59<03:29, 29.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.299270072992701\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6678832116788321\n",
      "min episode reward: -1.0\n",
      "total episodes: 70275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████                        | 5/11 [02:29<02:59, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.262206148282098\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6130198915009042\n",
      "min episode reward: -1.0\n",
      "total episodes: 70828\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████                    | 6/11 [02:58<02:29, 29.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.021164021164021\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6402116402116402\n",
      "min episode reward: -1.0\n",
      "total episodes: 71395\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████                | 7/11 [03:28<01:59, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.4025735294117645\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6764705882352942\n",
      "min episode reward: -1.0\n",
      "total episodes: 71939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████            | 8/11 [03:58<01:29, 29.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.219202898550725\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6485507246376812\n",
      "min episode reward: -1.0\n",
      "total episodes: 72491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████        | 9/11 [04:28<00:59, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.21981981981982\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.654054054054054\n",
      "min episode reward: -1.0\n",
      "total episodes: 73046\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████    | 10/11 [04:58<00:29, 29.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.312043795620438\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6824817518248175\n",
      "min episode reward: -1.0\n",
      "total episodes: 73594\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 11/11 [05:28<00:00, 29.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.165170556552963\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7019748653500898\n",
      "min episode reward: -1.0\n",
      "total episodes: 74151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▋                                        | 1/12 [00:29<05:28, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.189964157706093\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6523297491039427\n",
      "min episode reward: -1.0\n",
      "total episodes: 74709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████▎                                    | 2/12 [00:59<04:58, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.203971119133574\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6462093862815884\n",
      "min episode reward: -1.0\n",
      "total episodes: 75263\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████                                 | 3/12 [01:29<04:29, 29.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.366482504604051\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6906077348066298\n",
      "min episode reward: -1.0\n",
      "total episodes: 75806\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████▋                             | 4/12 [01:59<03:58, 29.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.292727272727273\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6509090909090909\n",
      "min episode reward: -1.0\n",
      "total episodes: 76356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████▎                         | 5/12 [02:29<03:29, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.163375224416517\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.651705565529623\n",
      "min episode reward: -1.0\n",
      "total episodes: 76913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 6/12 [02:59<02:59, 29.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.211849192100539\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6481149012567325\n",
      "min episode reward: -1.0\n",
      "total episodes: 77470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████████▋                  | 7/12 [03:29<02:29, 29.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.965217391304348\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6765217391304348\n",
      "min episode reward: -1.0\n",
      "total episodes: 78045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████▎              | 8/12 [03:59<02:00, 30.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.194244604316546\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6330935251798561\n",
      "min episode reward: -1.0\n",
      "total episodes: 78601\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████           | 9/12 [04:29<01:29, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.341911764705882\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6544117647058824\n",
      "min episode reward: -1.0\n",
      "total episodes: 79145\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████████▊       | 10/12 [04:59<00:59, 29.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.273550724637682\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6231884057971014\n",
      "min episode reward: -1.0\n",
      "total episodes: 79697\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████▍   | 11/12 [05:28<00:29, 29.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.106761565836299\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.697508896797153\n",
      "min episode reward: -1.0\n",
      "total episodes: 80259\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 12/12 [05:58<00:00, 29.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.949389179755672\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.675392670157068\n",
      "min episode reward: -1.0\n",
      "total episodes: 80832\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▋                                        | 1/12 [00:29<05:25, 29.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.1387900355871885\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6761565836298933\n",
      "min episode reward: -1.0\n",
      "total episodes: 81394\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████▎                                    | 2/12 [00:59<04:57, 29.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 6.941278065630398\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6511226252158895\n",
      "min episode reward: -1.0\n",
      "total episodes: 81973\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████                                 | 3/12 [01:29<04:27, 29.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.261904761904762\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.7142857142857143\n",
      "min episode reward: -1.0\n",
      "total episodes: 82519\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████▋                             | 4/12 [01:59<03:58, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.185714285714286\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6678571428571428\n",
      "min episode reward: -1.0\n",
      "total episodes: 83079\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████▎                         | 5/12 [02:29<03:29, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.175942549371634\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6229802513464991\n",
      "min episode reward: -1.0\n",
      "total episodes: 83636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 6/12 [02:59<02:59, 29.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.258589511754069\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6889692585895117\n",
      "min episode reward: -1.0\n",
      "total episodes: 84189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████████████████████████▋                  | 7/12 [03:29<02:29, 29.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.182468694096601\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6457960644007156\n",
      "min episode reward: -1.0\n",
      "total episodes: 84748\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████▎              | 8/12 [03:59<02:00, 30.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean episode length: 7.305147058823529\n",
      "max episode reward: 1.0\n",
      "mean episode reward: 0.6507352941176471\n",
      "min episode reward: -1.0\n",
      "total episodes: 85292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████▎              | 8/12 [04:03<02:01, 30.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m rpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mipe\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(trainer\u001b[38;5;241m.\u001b[39mget_weights()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_policy\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)[[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m]]\n\u001b[1;32m     33\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mforeach_env(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mupdate_weights(weights))\n",
      "Cell \u001b[0;32mIn [16], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(agent, n_iter, n_report)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(agent, n_iter, n_report):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_iter)):\n\u001b[0;32m----> 3\u001b[0m         report \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m n_report):  \n\u001b[1;32m      6\u001b[0m             \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/software/jupyter/lib/python3.8/site-packages/ray/tune/trainable/trainable.py:347\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    346\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 347\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/software/jupyter/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:661\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m     (\n\u001b[1;32m    654\u001b[0m         results,\n\u001b[1;32m    655\u001b[0m         train_iter_ctx,\n\u001b[1;32m    656\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/software/jupyter/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:2373\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 2373\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2375\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/Documents/software/jupyter/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo.py:407\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    404\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers, max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    405\u001b[0m     )\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m~/Documents/software/jupyter/lib/python3.8/site-packages/ray/rllib/execution/rollout_ops.py:100\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     97\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39msample()]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Update our counters for the stopping criterion of the while loop.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m sample_batches:\n",
      "File \u001b[0;32m~/Documents/software/jupyter/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/software/jupyter/lib/python3.8/site-packages/ray/_private/worker.py:2269\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an object ref \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a list of object refs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2266\u001b[0m     )\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2269\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   2271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/Documents/software/jupyter/lib/python3.8/site-packages/ray/_private/worker.py:669\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    668\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 669\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data, metadata) \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1211\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:173\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "trainer_config = {\n",
    "        \"model\": {\n",
    "            \"custom_model\": \"custom_agent\",\n",
    "            #'use_lstm':True,\n",
    "            },\n",
    "        \"framework\":'tf2',\n",
    "        #'eager_tracing':True,\n",
    "        \"num_workers\": 6,\n",
    "        'lr': .03,\n",
    "        'gamma': 1,\n",
    "        \n",
    "        #'train_batch_size':100\n",
    "         }\n",
    "\n",
    "ModelCatalog.register_custom_model('custom_agent', Agent)\n",
    "register_env(MyEnv, 'custom_env', env_config={})\n",
    "trainer = ppo.PPO(env='custom_env', config=trainer_config)\n",
    "#trainer = dqn.DQN(env='custom_env', config=trainer_config)\n",
    "\n",
    "\n",
    "n_epochs = 100000\n",
    "ipe = 1 # iterations per epoch\n",
    "rpt = 1\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    train(trainer, ipe+i//2, rpt)\n",
    "    #вытаскиваем веса обученной модели и присваием в сеть-соперника в каждого агента\n",
    "    weights = np.array(trainer.get_weights()['default_policy'], dtype='object')[[0,1,4,5,8,9]]\n",
    "    trainer.workers.foreach_env(lambda x: x.update_weights(weights))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(obs):\n",
    "    matrix = env.state['obs'].reshape(n_cols, n_cols).T[::-1].astype(str)\n",
    "    matrix[np.where(matrix=='0')] = '░'\n",
    "    matrix[np.where(matrix=='1')] = '▒'\n",
    "    matrix[np.where(matrix=='2')] = '▓'\n",
    "    print(matrix, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']] \n",
      "\n",
      "5\n",
      "[['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '▓' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']] \n",
      "\n",
      "5\n",
      "[['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '░' '░' '░' '▓' '░']\n",
      " ['░' '░' '▓' '░' '░' '▒' '░']] \n",
      "\n",
      "5\n",
      "[['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '▓' '░' '░' '▓' '░']\n",
      " ['░' '░' '▓' '░' '░' '▒' '░']] \n",
      "\n",
      "5\n",
      "[['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '▓' '░' '░' '▓' '░']\n",
      " ['░' '░' '▓' '▓' '░' '▒' '░']] \n",
      "\n",
      "5\n",
      "[['░' '░' '░' '░' '░' '░' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '░' '░' '░' '▒' '░']\n",
      " ['░' '░' '▓' '░' '░' '▓' '░']\n",
      " ['░' '░' '▓' '▓' '░' '▒' '░']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# тест  обученности\n",
    "env = MyEnv()\n",
    "env.act=1\n",
    "visualize(env.state['obs'])\n",
    "while not env.done:\n",
    "    \n",
    "    #env.done=True\n",
    "    if env.act==1:\n",
    "        action = int(input())\n",
    "    else: \n",
    "        action = env.make_opposite_action(env.state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    visualize(env.state['obs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Puzzles",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
